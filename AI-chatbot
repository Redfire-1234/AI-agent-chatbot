# install libraries
!pip install -q transformers accelerate bitsandbytes datasets peft sentencepiece

# download zip from huggingface
from huggingface_hub import login, hf_hub_download

login()

zip_path = hf_hub_download(
    repo_id="Redfire-1234/AI_agent",
    filename="AI_agent.zip",
    repo_type="dataset"
)

print("ZIP downloaded to:", zip_path)

# unzip and verify files
!unzip -o "$zip_path" -d ./AI_agent_data

import os
for root, dirs, files in os.walk("./AI_agent_data", topdown=False):
    for name in files:
        print(os.path.join(root, name))

# load and merge CSVs
import pandas as pd
import glob

path = "./AI_agent_data/*.csv"
files = glob.glob(path)

dfs = []
for file in files:
    df = pd.read_csv(file)
    platform = file.split("/")[-1].replace(".csv", "")
    df["platform"] = platform
    dfs.append(df)

final_df = pd.concat(dfs, ignore_index=True)
final_df.head()

# clean dataset
final_df.drop_duplicates(inplace=True)
final_df.dropna(subset=["text"], inplace=True)

import re
def clean_text(x):
    x = re.sub(r"http\S+", "", x)
    x = re.sub(r"[^\x00-\x7F]+", " ", x)
    return x.strip()

final_df["text"] = final_df["text"].astype(str).apply(clean_text)
final_df = final_df[final_df["text"].str.len() > 5]

final_df = final_df.rename(columns={"text": "prompt", "platform": "source"})
final_df.to_csv("ai_agent_cleaned.csv", index=False)

# create input/output pairs with better variety
import random

df = pd.read_csv("ai_agent_cleaned.csv")

def create_row(text, source):
    # Create varied instruction formats
    instructions = [
        f"Summarize this {source} post: {text}",
        f"What is this {source} post about? {text}",
        f"Explain this {source} content: {text}",
        f"Describe the main point of this {source} post: {text}",
        f"Analyze this {source} message: {text}"
    ]
    
    # Create more meaningful outputs based on content
    text_lower = text.lower()
    
    # Extract first part as summary
    summary = text[:80].strip()
    if len(text) > 80:
        summary += "..."
    
    # Detect sentiment
    if any(word in text_lower for word in ['love', 'amazing', 'great', 'awesome', 'excellent', 'fantastic']):
        sentiment = "positive"
    elif any(word in text_lower for word in ['hate', 'terrible', 'bad', 'awful', 'horrible', 'worst']):
        sentiment = "negative"
    else:
        sentiment = "neutral"
  # Create varied output formats
    output_templates = [
        f"This {source} post expresses a {sentiment} sentiment. Key point: {summary}",
        f"The main message is {sentiment} in tone. {summary}",
        f"Summary: {summary} (Sentiment: {sentiment})",
        f"This discusses {source} with a {sentiment} perspective. {summary}"
    ]
    
    inp = random.choice(instructions)
    out = random.choice(output_templates)
    
    return inp, out


df_chat = df.apply(lambda x: create_row(x["prompt"], x["source"]), axis=1, result_type="expand")
df_chat.columns = ["input", "output"]
df_chat.to_csv("chatbot_dataset.csv", index=False)

print("Sample training examples:")
print(df_chat.head(5))

# load dataset and sample
from datasets import load_dataset

dataset = load_dataset("csv", data_files="chatbot_dataset.csv")["train"]
dataset = dataset.train_test_split(test_size=0.1)

train_dataset = dataset["train"].select(range(min(3000, len(dataset["train"]))))
eval_dataset = dataset["test"].select(range(min(300, len(dataset["test"]))))

# tokenizer + tokenize function
from transformers import AutoTokenizer

model_name = "Qwen/Qwen2.5-1.5B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize_function(example):
    texts = [i + "\n" + o for i, o in zip(example["input"], example["output"])]
    tokens = tokenizer(
        texts,
        padding="max_length",
        truncation=True,
        max_length=256,
    )
    tokens["labels"] = tokens["input_ids"]
    return tokens

tokenized_train = train_dataset.map(tokenize_function, batched=True)
tokenized_eval = eval_dataset.map(tokenize_function, batched=True)

# load QloRA 4-bit model
import torch
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=quant_config,
    device_map="auto"
)

# apply LoRA
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model

model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)

lora_conf = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    bias="none",
    lora_dropout=0.05,
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_conf)
model.print_trainable_parameters()

# training arguments + trainer
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./my-chatbot-model",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=2,
    num_train_epochs=1,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=20,
    save_steps=2000,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval
)

trainer.train()

# Save PEFT/LoRA model and tokenizer
model.save_pretrained("./my-chatbot-model")
tokenizer.save_pretrained("./my-chatbot-model")

print("Model weights and tokenizer saved to ./my-chatbot-model")

!zip -r my_chatbot_model.zip my-chatbot-model

from google.colab import files
files.download("my_chatbot_model.zip")

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch
import re

# --------------------------
# Load tokenizer and model
# --------------------------
base_model_name = "Qwen/Qwen2.5-1.5B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(
    "./my-chatbot-model",
    fix_mistral_regex=True
)

base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    device_map="auto",
    dtype=torch.float16
)

model = PeftModel.from_pretrained(base_model, "./my-chatbot-model")
model.eval()

print(f"Model loaded successfully on: {model.device}")

# --------------------------
# Clean user input
# --------------------------
def clean_input(text):
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"@\w+", "", text)
    text = re.sub(r"#\w+", "", text)
    text = re.sub(r"[^\x00-\x7F]+", " ", text)
    return text.strip()

# --------------------------
# Generate chatbot response
# --------------------------
def generate_response(user_input, max_new_tokens=100, temperature=0.7):
    cleaned_input = clean_input(user_input)
    
    # Format as chat
    messages = [{"role": "user", "content": cleaned_input}]
    
    try:
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    except:
        prompt = f"User: {cleaned_input}\nAssistant:"
    
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=512
    ).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=True,
            top_p=0.9,
            top_k=50,
            repetition_penalty=1.15,
            no_repeat_ngram_size=3,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    # Decode only new tokens
    new_tokens = outputs[0][inputs['input_ids'].shape[1]:]
    response = tokenizer.decode(new_tokens, skip_special_tokens=True)
    
    # Clean response
    response = re.sub(r"@\w+", "", response)
    response = re.sub(r"#\w+", "", response)
    response = re.sub(r"http\S+", "", response)
    
    return response.strip()

# --------------------------
# Example usage
# --------------------------
print("\n" + "="*60)
print("TESTING CHATBOT")
print("="*60)

test_inputs = [
    "Summarize this tweet about Twitter: The new Twitter update is amazing!",
    "What do you think about AI?",
    "Tell me about recent developments in technology"
]

for user_input in test_inputs:
    print(f"\nUser: {user_input}")
    response = generate_response(user_input, temperature=0.7)
    print(f"Chatbot: {response}")


# --------------------------
# Interactive chat loop
# --------------------------
def chat_loop():
    print("\n" + "="*60)
    print("INTERACTIVE CHAT MODE (type 'quit' to exit)")
    print("="*60)
    
    while True:
        user_input = input("\nYou: ").strip()
        
        if user_input.lower() in ['quit', 'exit', 'q']:
            print("Goodbye!")
            break
        
        if not user_input:
            continue
        
        response = generate_response(user_input, temperature=0.7)
        print(f"Bot: {response}")

# Uncomment the line below to start interactive chat
chat_loop()

# Push your model weights to huggingface repo
from huggingface_hub import login

# Login to Hugging Face (enter your token)
login()

# Push model + tokenizer to HF hub
model.push_to_hub("AI-agent")
tokenizer.push_to_hub("AI-agent")

print("Model pushed to Hugging Face Hub at: https://huggingface.co/Redfire-1234/AI-agent")

# Once pushed, you can load it anywhere with:

# from peft import PeftModel
# from transformers import AutoModelForCausalLM, AutoTokenizer
   
# base = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")
# model = PeftModel.from_pretrained(base, "Redfire-1234/AI-agent")
# tokenizer = AutoTokenizer.from_pretrained("Redfire-1234/AI-agent")

# --------------------------
# Advanced: Batch inference
# --------------------------
def batch_generate(input_list, max_new_tokens=100, temperature=0.7):
    """Generate responses for multiple inputs at once"""
    responses = []
    
    print("\nProcessing batch inference...")
    for i, user_input in enumerate(input_list, 1):
        print(f"Processing {i}/{len(input_list)}...", end="\r")
        response = generate_response(user_input, max_new_tokens, temperature)
        responses.append(response)
    
    print(f"\n✓ Completed {len(input_list)} inferences")
    return responses

# Example batch inference
batch_inputs = [
    "Summarize this: AI is changing the world",
    "What's this about? Machine learning is fascinating",
    "Explain: Deep learning models are powerful"
]

# Uncomment to run batch inference
# batch_responses = batch_generate(batch_inputs)
# for inp, out in zip(batch_inputs, batch_responses):
#     print(f"\nInput: {inp}")
#     print(f"Output: {out}")

# --------------------------
# Utility: Save conversation history
# --------------------------
def save_conversation(conversation_history, filename="conversation.txt"):
    """Save chat history to file"""
    with open(filename, "w", encoding="utf-8") as f:
        for turn in conversation_history:
            f.write(f"User: {turn['user']}\n")
            f.write(f"Bot: {turn['bot']}\n")
            f.write("-" * 60 + "\n")
    print(f"✓ Conversation saved to {filename}")

# Example usage:
# conversation = [
#     {"user": "Hello", "bot": "Hi there!"},
#     {"user": "How are you?", "bot": "I'm doing well!"}
# ]
# save_conversation(conversation)
